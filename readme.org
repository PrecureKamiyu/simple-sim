* roadmap

** TODO 设置管理

在 utils 文件里面，需要读取本地设置文件
   - *Roadmap:*
     - Implement validation methods in =ConfigurationLoader= to check the validity of configuration parameters.
     - Add methods to save and load configurations from JSON or YAML files.

** TODO 任务管理

   - *Missing Sections:*
     - *Task Prioritization:* Implement a priority system for tasks.
     - *Task Error Handling:* Handle different types of task errors (e.g., timeouts, memory issues).

** TODO 设备管理

   - *Missing Sections:*
     - *Resource Management:* Manage device resources (e.g., CPU, memory) and ensure tasks do not exceed these limits.

** TODO 网络管理

   - *Missing Sections:*
     - *Network Topology:* Define and manage the network topology (e.g., mesh, star).
     - *Network Congestion Handling:* Handle network congestion and adjust task transfer delays accordingly.

** TODO offloading

     - *Offloading Strategies:* Develop and implement different offloading strategies based on device capabilities and task requirements.

** TODO 统计数据收集

I intend to just use in-disk file to store the statistics.

Implement methods in the =Device= class to track resource utilization.

*** 1. /Define Statistics Storage/

#+begin_src python
class Statistics:
    def __init__(self):
        self.device_utilization = {}
        self.task_delays = []

    def record_device_utilization(self, device_id, utilization):
        if device_id not in self.device_utilization:
            self.device_utilization[device_id] = []
        self.device_utilization[device_id].append(utilization)

    def record_task_delay(self, task_id, delay):
        self.task_delays.append((task_id, delay))

    def get_average_task_delay(self):
        if not self.task_delays:
            return 0
        total_delay = sum(delay for _, delay in self.task_delays)
        return total_delay / len(self.task_delays)

    def save_to_file(self, filename):
        with open(filename, 'w') as f:
            f.write(json.dumps({
                'device_utilization': self.device_utilization,
                'task_delays': self.task_delays
            }))

    @staticmethod
    def load_from_file(filename):
        with open(filename, 'r') as f:
            data = json.loads(f.read())
            stats = Statistics()
            stats.device_utilization = data['device_utilization']
            stats.task_delays = data['task_delays']
            return stats
#+end_src

*** 2. retrieve statistics

#+begin_src python
def analyze_simulation_results(filename):
    stats = Statistics.load_from_file(filename)
    print(f"Average Task Delay: {stats.get_average_task_delay()}")
    for device_id, utilizations in stats.device_utilization.items():
        print(f"Device {device_id} Utilization: {sum(utilizations) / len(utilizations)}")
#+end_src

* texts

** text one

首先通过其他的设计过程中我写的文档你应该可以对这个问和我的简单设计有一些了解了，那么下面就具体展开讲一下我觉得需要提到的点。

1. python 版本务必不低于3.8，不然 ~from __ future import annotations~ 没法用，虽然这个的功能是类内部可以用自身作为类型提示，没了也就是提示不好写但是没有提示写代码异常痛苦。我的版本是3.9，为了避免一些问题还是最好能3.9及以上。

2. module 中的代码基本上都是对于服务器之类的实体的抽象，因为只是一个小demo，所以并没有使用继承、接口之类的的东西，以后如果继续做的话可能还是要考虑，虽然我觉得python里这些东西不太好用。

3. module 中的类基本上只有属性，而且大多数属性暂时都没用到。关于策略我本来想像 java 里一样用泛化来实现同一类型的多种策略使用一个抽象，但是为了省事其实并没有做策略相关的东西，我的想法是可以传一个函数进去然后调用，但是函数的参数其实感觉没法写死，根据需求可能需要的数据差别很大，最后在 mystrategy 中写了一堆静态函数当策略，然后互相调用，这种方式其实很不好，不利于拓展，看你了。

4. 我的调度中以任务为中心，每个任务有一个 next_schedule_time，这个代表下次被调度的时间，这个调度和计算卸载中任务卸载的调度不同，是我的整个程序的调度，通过小根堆保证每次都能取出最近被调度的任务，近似认为两个调度之间其他的设备内存等占用不变，可以参考scheduling_strategy，还是挺好看出来的。每次调度都更新全局时间，但是我这策略里没啥实际用处。

5.每次调度后 next_schedule_time 变为下次调度时间且状态改变，下面是每个状态的描述和工作：

- ~CREATED~ 刚创建，此时 next_schedule_time = create_time
- WAIT_TO_UPLOAD 此时任务的传输计划已经安排好，传输的设计下面说，只是现在可能还没轮到它
- WAIT_TO_SCHEDULE 此时任务已经位于进行计算卸载的服务器，等待卸载，理论上可能也会有个卸载队列，但是我这里没考虑，假设到了立即就能卸载，有的话可以类比上传，当然这只是我的设计
- WAIT_TO_PROCESS 类比上传，执行计划已经安排好了
- WAIT_TO_DOWNLOAD 类比上传
- UPLOADING 传输过程中，也就是正常情况下WAIT_TO_UPLOAD的下个阶段，但是因为WAIT_TO_UPLOAD的时候传输过程已经计算好了，其实没干啥，具体可以看代码
- PROCESSING 类比传输
- DOWNLOADING 类比下载
- COMPLETED DOWNLOADING结束后就是完成，因为我这里只把传输到边缘设备叫DOWNLOADING
- UNFINISHED 一般情况下不会有这个状态
- REJECTED 拒绝服务，可能是因为内存不够了，或者卸载结果是卸载到哪儿都不满足时间限制。

6. 传输的设计，尽管我对信道搞了很多属性，因为考虑到多路复用、噪声干扰之类的，不过这些属性我的策略设计没用到，我假定信道的使用是竞争关系，先到先得，反映到程序上就是先被调度的需要使用信道的优先使用，我的信道有个 next_upload_time 和 next_download_time，就负责记录下次可以使用的时间，这也是为什么第五点里我说传输计划已经安排好了，因为WAIT 的时候其实已经用了信道啊，cpu 啊之类的，next 啥的已经更新成使用过后的时间了，服务器也有个 next_process_time，其过程和信道这个是一样的.

7.我忘了考虑存储，所有任务直接都视为在内存，你如果要设计得更细致可以考虑进去，其实我觉得他们俩也差不多，就是多了延迟。任务 process 过后占用的空间由输入大小变为输出大小，传输的话，WAIT 时发送端扔保留占用，ING 时两侧都有占用，ING 后自然发送端就不用保留了，这是我的策略里面代码的工作方式。

8.卸载策略的输出应该是一个执行任务的服务器

9.移动策略我这里没考虑，就当他们是静止的，传输策略就是 6 的设计，其他的暂时想不到了，有问题的话随时问我，我写的

** text 策略

1. init
2. start
3. move_strategy
4. task_generate_strategy
5. offloading_strategy
6. transmit_strategy
7. scheduler

** text 还需要考虑的问题

任务类型：
- 计算密集型任务：如图像处理、视频编码等，需要较高的计算资源。
- 数据密集型任务：如大数据分析、机器学习模型训练等，需要较大的存储和内存资源。
- 时间敏感型任务：如实时通信、在线游戏等，需要低延迟的通信和计算服务。

通讯形式：
- 无线传输：模拟不同频段、不同调制方式的无线通信，并考虑信号干扰、传输延迟等因素。
- 有线传输：模拟不同带宽、不同延迟的有线通信，并考虑网络拥塞、丢包等问题。

cost

- 运行成本
- 带宽成本
- 内存成本
- 存储成本（还没考虑存储）

** text 问题描述

计算架构：
常见的架构为两层架构和三层架构

1.两层架构将所有计算任务都放在边缘处理，适用于时间敏感的应用程序
2.三层架构适用于同时具有时间敏感任务和计算密集型任务的应用程序。时间敏感任务在边缘处理，计算密集型任务在云上执行

为此服务器类有如下特性：
有子服务器（如云服务器和边缘服务器，边缘服务器和他自身的虚拟化资源或管理的其他服务器）
有层级标识
有坐标，暂时只考虑二维坐标
服务器有手段访问它的父级、同级和子级服务器
有子级的服务器自身可以有执行任务能力，也可以只管调度和传输

卸载分类：
（基于卸载流程）

1.由边缘设备卸载到边缘服务器
2.由边缘设备卸载到云服务器
3.由边缘服务器卸载到自身所在集群的其他服务器
4.任务为可分割的，不同部分卸载到不同层，如一部分用于本地执行，一部分用于边缘服务器，一部分为云服务器，最后一部分被拒绝。

（基于卸载场景）
1.一个边缘设备对应一个边缘服务器
2.一个边缘设备对应多个边缘服务器，由边缘服务器决定是否卸载且卸载到哪个服务器
3.多个边缘设备对应一个边缘服务器
4.多个边缘设备对应多个边缘服务器

卸载模式：
1. 二进制模式：任务的数据集必须作为一个整体在 MEC 服务器上本地或远程执行。
2. 部分卸载模式：允许任务分区。任务被划分为几个组件，这些组件被卸载到EC服务器，或者一些组件在本地执行。适用于由多个并行段组成的一些复杂任务。可以分为并行和顺序两类，并行只需要把每一部分各自分区即可，顺序需要考虑任务的先后顺序，用用向无环图表示。

优化目标：找到最佳卸载比率，即卸载的比特与总比特的比率。

暂时只考虑二进制模式。

信道模型：
多址技术：FDMA（频分多址）、TDMA（时分多址）、CDMA（码分多址）、SDMA（空分多址）、OFDMA（正交频分多址）。
分为无干扰模型和有干扰模型，对此不是很了解

需要考虑传输速率，基站连接设备数量的限制，基站与基站、基站与设备应该有传输速率限制

计算卸载操作：
卸载决策：任务卸载到哪，任务怎么分区。
服务器选择：一对多场景，选择合适的服务器。
无线资源分配：分配给任务的频率、时间等。
传输功率设置：为任务传输设置适当的功率。
计算资源分配：本地、边缘的计算资源分配。
时隙划分：无线功率传输WPT场景。（EH、卸载）

计算卸载目标：
延迟最小化：传输延迟和执行延迟。
能源消耗最小化：卸载的情况下传输和执行所消耗的能源。
任务丢弃最小化：最小化由于资源不足而导致的任务丢弃。
计算速率最大化：能量和计算资源限制的情况下。
计算效率最大化：计算效率是总计算比特数除以消耗的能量。
支付最小化：ED必须为EC或云计算中使用的资源付费的场景。

** text 简单设计

#+begin_example
TASK_STATUS:
	CREATED
	UPLOADING
	PROCESSING
	DOWNLOADING
	COMPLETED
	UNFINISHED
	REJECTED
#+end_example

TASK_ERROR:
    NO_ERROR
    TIMEOUT
    REJECTED_DUE_TO_MEMORY
    REJECTED_DUE_TO_CHANNEL

TASK_TYPE:
    NORMAL （普通）
    PARALLEL （并行分片）
    PARALLEL_CHILD （并行分片子任务）
    SEQUENCE （顺序分片）
    SEQUENCE_CHILD （顺序分片子任务）

Task:
    id：number
    type: TASK_TYPE
    edge_device_id: number
    create_time: number
    input_size: number
    process_size: number
    output_size: number
    deadline: number
    status: TASK_STATUS
    process_server: Server类 （None为在边缘设备上执行，不执行卸载）
    upload_time: number (上传用时)
    download_time: number (下载用时)
    process_time: number (执行用时)
    complete_time: number (完成时间)
    error: TASK_ERROR
    tasks: Task[] (分片使用，并行父节点使用，为所有的任务，顺序子节点使用，为有向无环图中该任务的子节点)
    parent: Task (分片使用)
    DAG: (顺序分片父节点使用，有向无环图，暂时放这里，还没想好怎么实现)

EdgeDevice：
    id：number （id）
    x：number（坐标x）
    y：number（坐标y）
    cpu_speed：number（处理性能/周期每秒）
    task_queue：Task[] （任务队列）
    move_strategy: MoveStrategy (移动策略，待定)

EdgeDeviceManager：
    edge_devices: EdgeDevice[]
    task_generate_strategy: TaskGenerateStrategy (任务生产策略，待定)
    transmit_energy_coefficient: number (传输能量系数)
    process_energy_coefficient: number (执行能量系数)

Server类：
    id：number （id）
    children：server[] （子基站）
    coverage：number （覆盖率/m）
    level：number （层级，比如1就是云服务器，2就是边缘服务器，3就是边缘的边缘...）
    x：number（坐标x）
    y：number（坐标y）
    cpu_speed：number（处理性能/周期每秒）
    next_process_time: number (下次可以执行的时间)
    memory_size: number (内存大小)
    rest_memory: number (剩余内存)
    task_queue：Task[]
    ChannelManagers: dict[str,ChannelManager]

ServerManager类
    serves: Server[]
    direct_upload_servers: Server[] (边缘设备可以直连上传的服务器)

ChannelManager类：
    channel_bandwidth： number (信道带宽/Hz)
    M：number (信号状态数)
    baud：number （波特率）
    power： number （信号功率/W）
    noise：number （噪声功率/W）
    p/n：number （信噪比/dB，和power noise选择使用即可， = 10 * log10 p/n）
    speed：传输速率 (一般为2.0*10^8 m/s)
    is_full_duplex: bool (是全双工吗？)
    is_no_noise: bool (是无噪声吗？决定奈式准则还是香农公式)
    bandwidth: number (带宽/bps = baud * channel_bandwidth *log2(M) | channel_bandwidth * log2(1+s/n))
    network_strategy: NetworkStrategy (网络策略，对于数据传输过程进行实现，具体怎么搞待定)

    channels: Channel[]
    all_bandwidth: number
    rest_bandwidth: number
    upload_delay: number
    download_delay: number

Channel:
    task_queue：Task[]
    upload_to: number (上传对象)
    download_from: number (下载对象)
    next_upload_time: number (对于全双工：下次可以上传的时间)
    next_download_time: number (对于全双工：下次可以下载的时间)
    next_time: number (对于半双工：下次可以使用的时间)
    next_confirmed_start_time: number (对于半双工：下次已确定任务的开始时间，即对于半双工如果后面已经有了确定的任务，
                                      只能利用这个间隙执行其他任务，可以具体任务队列具体调度)

Scheduler：
    time：number (当前时间，指的是模拟器内部模拟的时间)
    edge_device_manager: EdgeDeviceManager
    server_manager: ServerManager
    finished_task: Task[]
    scheduling_task: Task[] (刚生成和已完成之外的都在这里)
    scheduling_strategy: SchedulingStrategy (调度策略，选出卸载位置，具体怎么搞待定)

一般情况下一次完整卸载过程如下：
    1.任务产生
    2.任务上传至服务器
    3.服务器选择卸载位置或者自己计算
    4.任务传输至卸载至的服务器 （可选）
    5.结果传输至可传输至用户的服务器（可选）
    6.传输结果到用户

由一个调度器对所有的任务进行调度，每个任务有多个状态，每个任务记录下一次被调度的的时间，每次调度最近需要被调度的任务，可以认为两次调度之间资源状态一致
** 我的思路

*** prompt one

我需要设计一个 MEC 网络的模拟器，现在来说，我们的类有这些：

- Server
- ServerManager
- EdgeDevice
- EdgeDeviceManager
- Channel
- ChannelManager

我的主要疑惑点在于该如何实现这些设备的管理和设备之间的互动：

- 计算任务是什么时候、如何分配到 device 上面的
- 计算任务可以向服务器 offloading，这个 offloading 的具体细节，比如说哪些任务 offloading、什么时候 offloading 的？
- 计算任务的 offloading 的具体细节又是谁指定的？是设备自己还是有一个 offloading 管理器？

*** prompt two

我对计算任务的模拟有一些疑惑，比如说
- 计算任务是由谁来分配的？
- 如何模拟 Device 或者是 Server 处理计算任务的过程？可能每个 Device 内部都有一个计时器？每次需要解决计算任务的时候就会为这个计时器加时间？
- 模拟器的 main 程序的逻辑大概是什么样的？

*** prompt three

我对 main 程序有一些疑惑

- 我想要节点们完成任务的时候输出日志到本地文件之中用于分析
- main 程序按照你的描述可以是一个对计算任务队列的调度的循环。但是我的想法是能不能让 main 程序是一个 ticker，每隔一段时间间隔就发送一个事件给其他的节点？

*** 设计

- 调度器：主要程序，ticker，发送任务消息给管理器们
- 任务生成器：发送给 ticker 下一个时刻的任务需要、分配
- EdgeDevice 管理器：管理 EdgeDevice 们
- Server 管理器：管理 Server 们
- 卸载管理器：拥有所有上面两个管理器的所有 context，为卸载进行决策，发送消息

*** prompt four

我的模拟器其实并不是这个逻辑，scheduler 并不是真的物理实体，发送任务也只是 “虚拟” 地发送。传输只发生在 device 和服务器之间的通信。因为 offloading 有些计算任务会被传输到服务器上面，然后服务器再返回结果传输回来，或者将结果发送到指定的位置。只有在这个过程之中我需要考虑网络传输。请问在这种情况之下我该如何设计网络模块
** network delay calculation

*** prompt

I think the calculation of delay can be improved more. NetworkManager has the context of the devices. So network manager have the information of devices. What I want to do is that the module has all the information of the devices, and the connection situation between devices. NetworkManager should utilize those information for calculation of delay

Absolutely, leveraging the full context of devices and their connections can significantly improve the accuracy of delay calculations. Here's a more comprehensive approach that takes into account the network topology, device capabilities, and current load on each device and connection.

*** Detailed Delay Calculation with Full Context

1. /Base Delay Calculation:/
   - Calculate the base delay as the ratio of the data size to the available bandwidth.
   \[ \text{Base Delay} = \frac{\text{Data Size}}{\text{Bandwidth}} \]

2. /Device Load Factor:/
   - Consider the current load on the source and destination devices.
   \[ \text{Device Load Factor} = \text{Source Device Load} + \text{Destination Device Load} \]

3. /Connection Quality Factor:/
   - Account for the quality of the connection between the source and destination devices.
   \[  \text{Connection Quality Factor} = \text{Connection Quality} \times \text{Data Size} \]

4. /Queue Length Factor:/
   - Consider the number of tasks currently in the message queue of the devices.
   \[ \text{Queue Length Factor} = \text{Queue Length} \times \text{Per Task Delay} \]

5. /Network Load Factor:/
   - Account for the current network load, which can be estimated based on the number of active tasks and their data sizes.
   \[ \text{Network Load Factor} = \text{Total Active Data Size} \times \text{Load Factor} \]

6. /Random Factor:/
   - Add a random factor to simulate network variability.
   \[ \text{Random Factor} = \text{random.uniform(0, 0.1)} \]

7. /Total Delay:/
   - Sum all the factors to get the total delay.
   \[ \text{Total Delay} = \text{Base Delay} + \text{Device Load Factor} + \text{Connection Quality Factor} + \text{Queue Length Factor} + \text{Network Load Factor} + \text{Random Factor} \]

*** Implementation in =NetworkManager=

Here's how you can implement the detailed delay calculation in the =NetworkManager= class:

#+begin_src python
import random

class NetworkManager:
    def __init__(self, edge_device_context: EdgeDeviceManagerContext, server_context: ServerManagerContext):
        self.edge_device_context = edge_device_context
        self.server_context = server_context
        self.connections = self.initialize_connections()

    def initialize_connections(self):
        # Example: Initialize connections with quality values
        connections = {}
        for edge_device in self.edge_device_context.vm_list:
            for server in self.server_context.server_list:
                connections[(edge_device.device_id, server.device_id)] = random.uniform(0.5, 1.0)
        return connections

    def calculate_delay(self, data_size, bandwidth, src_device_id, dst_device_id) -> float:
        """
        Calculate the delay for transferring data over the network.

        :param data_size: Size of the data to be transferred.
        :param bandwidth: Available bandwidth for the transfer.
        :param src_device_id: ID of the source device.
        :param dst_device_id: ID of the destination device.
        :return: Calculated delay.
        """
        # Base delay due to data size and bandwidth
        base_delay = data_size / bandwidth

        # Device load factor
        src_device = next(device for device in self.edge_device_context.vm_list if device.device_id == src_device_id)
        dst_device = next(device for device in self.server_context.server_list if device.device_id == dst_device_id)
        device_load_factor = src_device.utilization + dst_device.utilization

        # Connection quality factor
        connection_quality = self.connections.get((src_device_id, dst_device_id), 1.0)
        connection_quality_factor = connection_quality * data_size

        # Queue length factor
        queue_length = src_device.tasks.qsize() + dst_device.tasks.qsize()
        per_task_delay = 0.01  # Example value, can be adjusted
        queue_length_factor = queue_length * per_task_delay

        # Network load factor
        total_active_data_size = sum(task.process_size for device in self.edge_device_context.vm_list for task in device.tasks.queue)
        load_factor = 0.01  # Example value, can be adjusted
        network_load_factor = total_active_data_size * load_factor

        # Random factor to simulate network fluctuations
        random_factor = random.uniform(0, 0.1)

        # Total delay
        total_delay = (base_delay + device_load_factor + connection_quality_factor +
                       queue_length_factor + network_load_factor + random_factor)

        return total_delay
#+end_src

*** Usage Example

Here's how you can use the =calculate_delay= method in the =TaskManager= class:

#+begin_src python
class TaskManager:
    def __init__(self, network_manager: NetworkManager, task_number: int, task_size: int):
        self.network_manager = network_manager
        self.task_size = task_size
        self.task_number = task_number
        self.current_id = -1
        self.task_count = 0
        self.statistics = Statistics()

    def generate_tasks(self) -> list[Task]:
        ret = []
        self.task_count += 10
        for _ in range(10):
            task = self.generate_one_task()
            ret.append(task)
            # Calculate delay for each task
            src_device_id = task.src_device
            dst_device_id = task.dst_device
            delay = self.network_manager.calculate_delay(task.process_size, 100, src_device_id, dst_device_id)
            self.statistics.record_task_delay(task.task_id, delay)
        logging.info(f"Generated {len(ret)} tasks.")
        return ret
#+end_src
